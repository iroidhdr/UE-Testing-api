# Local LLM Configuration (Ollama)
# No API key needed â€” Ollama runs locally
# Make sure Ollama is running: ollama serve
# Make sure llama3 is installed: ollama pull llama3
